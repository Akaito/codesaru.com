---
date:       2017-08-05 11:53 -07:00
title:      ASRN2
categories: noise
slug:       asrn2-0
published:  false
---

Take two on Accounting Software Right Now.

<!-- more -->

Wanted a reason to tinker with AWS to help with creating new material for [ZeroToBoto](https://akaito.github.io/ZeroToBoto/).
Picked up one of my projects from almost one year ago, accounting-software-right-now, to see how it might fit.
That project transformed bank OFX/CSV inputs into a custom .asrn, auto-filling in matched data for payees, categories, etc.
Heavily inspired by ledger-cli, which I wish just worked for me, but it doesn't quite.
Ultimately I'd like to replace my use of YNAB with a custom tool.
Tired of Intuit buying competing software and spoiling it.

Currently have:
- Script to create the "bank-out" bucket.<br/>
    Holds versioned uploads of OFX, etc. data downloaded from banks.
    This is intended to hold onto copies of bank data forever, in case anything ever needs it again.
- Script to create the "utd" (universal transaction data) bucket.<br/>
    UTD is some arbitrary format (not my old .asrn) that will be flexible enough to hold any bank output.
    It's pretty much just going to be a very generic key-value store with some set of keys I can expect for my own ease of use.
- Script to upload globbed *.ofx files to that first bucket.

All of the above import a configuration Python script that holds AWS profile name for .aws/credentials, any directory names I may want to change in the future, etc.
"Asset" files like some-bucket-policy.json are loaded and have "{{KEYS_LIKE_THIS}}" replaced as found in a special dictionary in the config Python, for easier reusability.
I want it to be extremely easy to create extra branches/sandboxes/etc.
Maybe someday even support other users, but I need to avoid getting distracted by that.

To consider in the future: should this all be done through a CURL-able endpoint to reduce being so strongly tied to AWS?
For now, just continuing to do whatever's easiest toward getting this stuff done.

Also tried to tinker with Lambda a bit to make a function which would fire on OFX upload to do the transform.
Ran into various issues there, some possibly due to the NoScript Firefox plugin blocking one part of the AWS console, and I didn't notice it for a bit.
However, I want to reduce dependence on auto-firing triggers anyway.
Ideally it will always be possible to progress from any state (all represented in files or simple enough file-like things), to the next desirable state.

---
### S3 Bucket creation
```python {% raw %}
#!/usr/bin/python3
# 005_prepare-bucket-bank-out.py


import botocore
import boto3

import asrnc_config


#================================================
def apply_policy(policy_obj, template):
    print('  Applying policy to bucket...')
    policy_str = ''
    with open(template, 'rt') as f:
        policy_str = f.read()
    for k,v in asrnc_config.TEMPLATE_VALUES.items():
        policy_str = policy_str.replace('{{{{{}}}}}'.format(k), v)
    #print(policy_str)
    policy_str = policy_str.replace(' ', '')
    policy_str = policy_str.replace('\n', '')

    policy_obj.put(Policy=policy_str)


#================================================
def apply_tags(bucket_tagging):
    print('  Tagging bucket...')
    bucket_tagging.put(
            Tagging={
                'TagSet': [
                    { 'Key': 'purpose', 'Value': 'asrnc' },
                    { 'Key': 'iam-owner', 'Value': asrnc_config.IAM_USERARN },
                    ]
                }
            )


#================================================
def main():
    """Create the bucket to store OFX, etc. files in forever."""

    session = boto3.Session(profile_name=asrnc_config.AWS_PROFILE_NAME_ROOT)
    s3 = session.resource('s3')
    bucket = s3.Bucket(asrnc_config.S3_BUCKET_NAMES['bank-out'])

    while True:  # keep trying until we succeed
        try:
            bucket.load()
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] != '404':
                break  # bucket exists
        else:
            break

        # create bucket
        print('Creating bucket... {}'.format(bucket.name))
        if asrnc_config.AWS_REGION == 'us-east-1':
            response = bucket.create()
        else:
            response = bucket.create(
                    ACL='private',
                    CreateBucketConfiguration={
                        'LocationConstraint': asrnc_config.AWS_REGION,
                        },
                    GrantFullControl='akai.to',  # TODO : Figure out the more proper form of this.
                    )
    print('Bucket exists: {}'.format(bucket.name))

    # further bucket setup
    apply_tags(bucket.Tagging())
    apply_policy(bucket.Policy(), 'aws/policies/bucket-bank-out.json')
    print('  Enabling versioning...')
    bucket.Versioning().enable()

    print('Bucket prepared at {}'.format(bucket.name))

if __name__ == '__main__':
    main()
{% endraw %}```

---
### S3 Bucket policy template
`s3:ListBucket` is applied to the first resource (the bucket itself).
All the other actions are applied to the second resource; every object within the bucket.
```json {% raw %}
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "AWS": "{{IAM_USER}}"
            },
            "Action": [
                "s3:GetObject",
                "s3:ListBucket",
                "s3:PutObject",
                "s3:PutObjectTagging"
            ],
            "Resource": [
                "arn:aws:s3:::{{BANK_DATA_BUCKET}}",
                "arn:aws:s3:::{{BANK_DATA_BUCKET}}/*"
            ]
        }
    ]
}

{% endraw %}```

---
### OFX file upload
```python {% raw %}
#!/usr/bin/python3
# 010_upload-bank-output.py

import datetime
import glob
import os

import botocore
import boto3

import asrnc_config


#================================================
def main():
    session = boto3.Session(profile_name=asrnc_config.AWS_PROFILE_NAME)
    s3 = session.resource('s3')
    indir = asrnc_config.DIRECTORIES['bank-out']
    today = datetime.date.today()
    key_format = '{}-{}/{}'

    # scan dir for .ofx files
    for infile in glob.glob(indir + '*.ofx'):
        local_modtime = datetime.datetime.fromtimestamp(
                os.path.getmtime(infile),
                tz=datetime.timezone.utc
                )

        s3_key = '{}-{:02}/{}'.format(today.year, today.month, os.path.basename(infile))
        s3_object = s3.Object(asrnc_config.S3_BUCKET_NAMES['bank-out'], s3_key)

        # keep trying to upload until we succeed
        while True:
            s3_modtime = datetime.datetime.fromtimestamp(0, tz=datetime.timezone.utc)
            # check s3 last mod time (if object exists)
            try:
                s3_modtime = s3_object.last_modified
            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] != '404':
                    raise

            # done when local file is older than S3 object's modtime
            if local_modtime <= s3_modtime:
                print('OFX file on S3 is up to date: {}'.format(infile))
                break

            print('Uploading OFX to S3: {}'.format(infile))
            # upload file to S3 with tags set
            with open(infile, 'rb') as f:
                s3_object.put(
                        Body=f,
                        Tagging='state=just_uploaded'  # Probably won't use this.
                        )

            s3_object.load()


if __name__ == '__main__':
    main()
{% endraw %}```

